**BiomedCLIP** by Zhang et al. (2025) represents a breakthrough in biomedical vision-language modelling with **15 million image-text pairs** from PubMed Central, creating the largest and most diverse biomedical multimodal dataset. The model adapts CLIP architecture with **PubMedBERT text encoder** and **Vision Transformer (ViT-B/16)** optimised for biomedical domain.

**Technical architecture specifications** include 768-dimensional embeddings from ViT-B/16 with 224Ã—224 input resolution, 256-token context length, and InfoNCE contrastive loss. The model achieves **73.5% text-to-image recall** and substantially outperforms general CLIP on biomedical tasks (73.41% vs. baseline performance on pathology datasets).

For **MRI feature extraction in epilepsy prediction**, BiomedCLIP provides several critical advantages: semantic understanding of neuroimaging terminology ("ASL - Arterial Spin Labelling"), anatomical localisation capabilities, and cross-modal alignment between visual features and clinical descriptions. The **zero-shot learning capability** is particularly valuable, enabling processing of novel epilepsy-related imaging without task-specific fine-tuning.

**Integration into multimodal frameworks** occurs through the vision encoder's 768-dimensional feature vectors that capture both coarse-grained (modality type) and fine-grained (specific pathological findings) information. These embeddings provide interpretable representations that can be combined with EEG features, genomic data, and clinical variables for comprehensive epilepsy prediction.

---

_This summary was generated by Claude Opus 4 (Anthropic)_

---