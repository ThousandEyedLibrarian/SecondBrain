**wav2vec 2.0** by Baevski et al. (2020) revolutionised speech recognition through masked latent space modelling and provides the architectural foundation for EEG Q-Net systems in [[Epilepsy]] prediction. The paper's core innovation lies in its **end-to-end [[Contrastive Learning]] framework** that learns powerful representations from unlabelled audio data, achieving state-of-the-art performance with 100x less labelled data than previous approaches.

The architecture employs a **7-layer CNN feature encoder** followed by a **[[Transformer]] context network** (12-24 layers) that processes masked time steps through contrastive learning. The key technical breakthrough is **joint learning of discrete and continuous representations** - raw audio feeds the CNN encoder to produce latent representations, while a quantisation module creates discrete targets for contrastive learning. This hybrid approach enables the model to capture both fine-grained temporal patterns and higher-level semantic structures.

For epilepsy applications, this framework directly transfers to [[Electroencephalography (EEG)]] signal processing through **BENDR** and similar adaptations. The masking strategy (~49% of time steps) teaches the model temporal dependencies crucial for [[Seizure]] prediction, while the contrastive objective helps distinguish pre-ictal from interictal EEG patterns. **The architecture's ability to learn from minimally labelled EEG data is particularly valuable in medical contexts** where annotated seizure data is scarce and expensive to obtain.

Training requires substantial computational resources (64-128 V100 GPUs), but the resulting [[Embeddings]] achieve remarkable performance: 4.8/8.2 WER on LibriSpeech with only 10 minutes of labelled data. When adapted to EEG, similar architectures show >90% accuracy in seizure detection tasks while maintaining real-time processing capability essential for clinical deployment.

---

_This summary was generated by Claude Opus 4 (Anthropic) and amended by me._

---