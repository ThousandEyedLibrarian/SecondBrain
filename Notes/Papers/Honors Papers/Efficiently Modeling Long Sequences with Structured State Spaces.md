This paper addresses the fundamental challenge of long-range dependencies in sequence modelling, where traditional approaches including RNNs, CNNs, and [[Transformer]]s struggle to scale beyond sequences of a few thousand steps. The authors introduce **S4 (Structured State Spaces)**, a novel architecture based on state space models (SSMs)—mathematical systems from [[Control Theory]] defined by continuous-time equations $x'(t) = Ax(t) + Bu(t) and y(t) = Cx(t)$. The key innovation lies in using **HiPPO (High-order Polynomial Projection Operators) matrices** as the state matrix A, which mathematically enables memorisation of long-range dependencies by decomposing input signals onto orthogonal basis functions with exponentially-decaying measure. However, direct application of HiPPO matrices proved computationally intractable due to numerical instabilities when diagonalised. S4 solves this through a **Normal Plus Low-Rank (NPLR) parameterisation**, decomposing HiPPO matrices as $A = VΛV* - PQ^T$, where the matrix can be unitarily diagonalised whilst maintaining a low-rank correction term. This structure enables efficient computation by converting the SSM from time domain to frequency domain using generating functions, applying the Woodbury identity for the low-rank term, and reducing the core computation to well-studied Cauchy kernel evaluations with $O(N + L)$ complexity.

The theoretical framework delivers exceptional empirical performance across diverse benchmarks, achieving 86.09% average accuracy on the Long Range Arena benchmark compared to less than 60% for all Transformer variants, with S4 being the first model to solve the challenging Path-X task (96.35% accuracy on 16K-length sequences). On sequential CIFAR-10, S4 achieves 91.13% accuracy using no 2D inductive bias, competitive with specialised 2D CNNs, whilst on raw speech classification it reaches 98.3% accuracy, outperforming hand-crafted preprocessing approaches. The model's dual computational modes—operating as either a recurrence for $O(1)$-per-step autoregressive generation or a convolution for efficient parallel training—enable 60× faster generation than standard autoregressive models whilst matching Transformer language modelling performance within 0.8 perplexity on WikiText-103. S4's practical impact extends beyond long-range dependency tasks, offering a unified framework that bridges continuous-time models, [[Recurrent Neural Networks (RNN)]], and [[Convolutional Neural Networks (CNN)]] whilst scaling linearly rather than quadratically with sequence length. This work represents a fundamental shift towards mathematically principled sequence architectures, demonstrating how classical control theory can be adapted for modern [[Deep Learning]] to create more efficient and theoretically grounded models for long-sequence tasks across text, audio, images, and time series domains.

---

_This summary was generated by Claude Sonnet 4 and amended by me._