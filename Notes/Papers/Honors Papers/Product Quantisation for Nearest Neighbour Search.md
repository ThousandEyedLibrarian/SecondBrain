This paper addresses the fundamental challenge of approximate nearest neighbour (ANN) search in high-dimensional spaces, where traditional methods like Locality-Sensitive Hashing (LSH) and FLANN suffer from prohibitive memory requirements when scaling to large datasets. The authors introduce **product quantisation**, a novel approach that decomposes $D$-dimensional vectors into m distinct subvectors of dimension $D*/m$, then quantises each subspace separately using k-means clustering. This creates a compact representation where vectors are encoded as short codes composed of subspace quantisation indices, enabling efficient Euclidean distance estimation through pre-computed lookup tables. 

The key technical contribution is the **Asymmetric Distance Computation (ADC)** method, which quantises only database [[Vectors]] while keeping queries unquantised, significantly reducing quantisation noise compared to symmetric approaches. The method is further enhanced with **IVFADC** (Inverted File with ADC), combining product quantisation with inverted file structures to avoid exhaustive search whilst maintaining accuracy.

This is the basis of the system [[wav2vec 2.0 - a framework for self-supervised learning of speech representations]] uses for bin quantisation ([[Neural Codebook (NC)]]).

Experimental validation on SIFT (128-dimensional) and GIST (960-dimensional) descriptors demonstrates substantial improvements over state-of-the-art methods. Using parameters $m=8$ and $k*=256$ (producing 64-bit codes), the approach achieves superior recall@100 (?) performance compared to spectral hashing, with IVFADC requiring only 25MB of [[Memory]] versus FLANN's 250MB whilst delivering better precision across multiple operating points. 

The method's scalability is convincingly demonstrated on a dataset of 2 billion SIFT descriptors, showing consistent performance gains. The practical impact is significant for large-scale image retrieval systems, enabling efficient search through billions of local descriptors with minimal memory overheadâ€”a critical requirement for real-world applications like visual search engines and content-based image retrieval systems where storing original high-dimensional vectors becomes computationally intractable.

For our intents and purposes we care because we can use it for MRI imaging and [[Electroencephalography (EEG)]] data.


See also:
- [[K-Nearest Neighbours Algorithm]]

---

_This summary was generated by Claude Sonnet 4 and amended by me._